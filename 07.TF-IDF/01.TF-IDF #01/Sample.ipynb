{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb6d31fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import joblib\n",
    "import numpy as np\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ee9daf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_matrix.joblib']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MeCabの初期化\n",
    "mecab = MeCab.Tagger(\"\")\n",
    "\n",
    "# サンプルの日本語文章\n",
    "documents = [\n",
    "    \"これはテスト文章です。\",\n",
    "    \"もう一つのサンプル文です。\",\n",
    "    \"形態素解析を行い、Doc2Vecモデルを作成します。\"\n",
    "]\n",
    "\n",
    "# 形態素解析を行う関数（特定の品詞を抽出）\n",
    "def tokenize(text):\n",
    "    node = mecab.parseToNode(text)\n",
    "    tokens = []\n",
    "    while node:\n",
    "        feature = node.feature.split(',')\n",
    "        if feature[0] in ['名詞', '動詞', '形容詞']:  # 名詞、動詞、形容詞のみ抽出\n",
    "            tokens.append(node.surface)\n",
    "        node = node.next\n",
    "    return tokens\n",
    "\n",
    "# ドキュメントを形態素解析し、タグ付きドキュメントとして格納\n",
    "tagged_data = []\n",
    "for i, doc in enumerate(documents):\n",
    "    tokens = tokenize(doc)  # ドキュメントをトークン化\n",
    "    tagged_document = TaggedDocument(words=tokens, tags=[str(i)])  # トークンとタグを用いてTaggedDocumentを作成\n",
    "    tagged_data.append(tagged_document)  # tagged_dataリストに追加\n",
    "\n",
    "# TF-IDF モデルの作成\n",
    "corpus = [\" \".join(td.words) for td in tagged_data]\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# インスタンスの保存\n",
    "joblib.dump(vectorizer, \"vectorizer.joblib\")\n",
    "joblib.dump(tfidf_matrix, \"tfidf_matrix.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad6fedfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0 Similarity: 0.7071\n",
      "Document 1 Similarity: 0.0000\n",
      "Document 2 Similarity: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# インスタンスのロード\n",
    "vectorizer = joblib.load(\"vectorizer.joblib\")\n",
    "tfidf_matrix = joblib.load(\"tfidf_matrix.joblib\").toarray()\n",
    "\n",
    "# 新しいドキュメントの類似度を計算\n",
    "new_document = \"これは新しい文章です。\"\n",
    "new_vector = vectorizer.transform([\" \".join(tokenize(new_document))]).toarray()[0]\n",
    "\n",
    "# 既存のドキュメントと新しいドキュメントの類似度を計算\n",
    "similarities = []\n",
    "for i in range(len(documents)):\n",
    "    doc_vector = tfidf_matrix[i]\n",
    "    similarity = np.dot(new_vector, doc_vector) / (\n",
    "                 np.linalg.norm(new_vector) * np.linalg.norm(doc_vector))\n",
    "    similarities.append((i, similarity))\n",
    "\n",
    "# 類似度で降順ソート\n",
    "similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 類似度の結果を表示\n",
    "for idx, similarity in similarities:\n",
    "    print(f\"Document {idx} Similarity: {similarity:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
