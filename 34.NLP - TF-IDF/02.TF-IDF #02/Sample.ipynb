{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb6d31fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import MeCab\n",
    "import pickle\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ee9daf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_matrix.joblib']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# データの作成\n",
    "# ---------------------------------------------------------\n",
    "def create_data(author_name, txt_files):\n",
    "  txt_dir = 'txt_' + author_name\n",
    "  book_list = []\n",
    "  content_list = []\n",
    "\n",
    "  # ファイルの読み込みとコンテンツの保存\n",
    "  for file_name in txt_files:\n",
    "    file_path = os.path.join(txt_dir, file_name)\n",
    "    with open(file_path, 'r', encoding='shift-jis') as file:\n",
    "      book_list.append(file.readline().strip())                   # 書籍名を取得\n",
    "      content_list.append(tokenize(sanitize(file.read())))        # 不要な部分を削除したコンテンツのトークンを取得\n",
    "\n",
    "  return book_list, content_list\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# コンテンツの不要な部分を除外\n",
    "# ---------------------------------------------------------\n",
    "def sanitize(text):\n",
    "  operations = [\n",
    "    lambda text: re.split(r'\\-{5,}', text)[2],\n",
    "    lambda text: re.split(r'底本：', text)[0],\n",
    "    lambda text: re.sub(r'《.+?》', '', text),\n",
    "    lambda text: re.sub(r'［＃.+?］', '', text),\n",
    "    lambda text: text.strip()\n",
    "  ]\n",
    "\n",
    "  for operation in operations:\n",
    "    try:\n",
    "      text = operation(text)\n",
    "    except Exception as e:\n",
    "      pass\n",
    "\n",
    "  return text\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 形態素解析\n",
    "# ---------------------------------------------------------\n",
    "def tokenize(text):\n",
    "  tagger = MeCab.Tagger(\"\")\n",
    "  node = tagger.parseToNode(text)\n",
    "  tokens = []\n",
    "\n",
    "  while node is not None:\n",
    "    part = node.feature.split(\",\")[0]\n",
    "    if part in [\"名詞\", \"動詞\", \"形容詞\"]:\n",
    "      tokens.append(node.surface)\n",
    "    node = node.next\n",
    "\n",
    "  return tokens\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# メイン\n",
    "# ---------------------------------------------------------\n",
    "with open(\"book_data.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "author_list = data[\"author_list\"]\n",
    "file_list   = data[\"file_list\"]\n",
    "\n",
    "tagged_data = []\n",
    "\n",
    "# データの作成\n",
    "for i in range(len(author_list)):\n",
    "  book_list, content_list = create_data(author_list[i], file_list[i])\n",
    "  for j in range(len(book_list)):\n",
    "    tag = f'{author_list[i]}：{book_list[j]}'\n",
    "    tagged_data.append({'tag': tag, 'content': ' '.join(content_list[j])})\n",
    "\n",
    "# TF-IDF モデルの作成\n",
    "corpus = [d['content'] for d in tagged_data]\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# インスタンスの保存\n",
    "joblib.dump(vectorizer, \"vectorizer.joblib\")\n",
    "joblib.dump(tfidf_matrix, \"tfidf_matrix.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad6fedfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "宮沢 賢治：〔青びかる天弧のはてに〕：0.5338304133207578\n",
      "宮沢 賢治：ありときのこ：0.026369219333433738\n",
      "宮沢 賢治：青柳教諭を送る：0.025855718301230894\n",
      "北原 白秋：思ひ出：0.02568242095534178\n",
      "芥川 竜之介：芥川龍之介歌集：0.022983236274996113\n",
      "宮沢 賢治：或る農学生の日誌：0.022447943278962598\n",
      "宮沢 賢治：秋田街道：0.02041374320760784\n",
      "ポー エドガー・アラン：ウィリアム・ウィルスン：0.0198302554455171\n",
      "ポー エドガー・アラン：アッシャー家の崩壊：0.019343266417246836\n",
      "芥川 竜之介：愛読書の印象：0.017117252667950132\n"
     ]
    }
   ],
   "source": [
    "# 新しいテキストのベクトル化と類似度の計算\n",
    "new_text = '青びかる天弧のはてに、きらゝかに町はうかびて、六月のたつきのみちは、いまやはた尽きはてにけり。いさゝかの書籍とセロを、思ふまゝ'\n",
    "new_text_tokens = tokenize(new_text)\n",
    "new_text_vector = vectorizer.transform([' '.join(new_text_tokens)])\n",
    "\n",
    "# コサイン類似度の計算\n",
    "cosine_similarities = cosine_similarity(new_text_vector, tfidf_matrix).flatten()\n",
    "similar_indices = cosine_similarities.argsort()[:-11:-1]\n",
    "\n",
    "# 結果の表示\n",
    "for idx in similar_indices:\n",
    "  print(f\"{tagged_data[idx]['tag']}：{cosine_similarities[idx]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
