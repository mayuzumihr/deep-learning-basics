{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os.path\n",
    "import urllib.request as req\n",
    "import MeCab\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 20\n",
      "iteration 21\n",
      "iteration 22\n",
      "iteration 23\n",
      "iteration 24\n",
      "iteration 25\n",
      "iteration 26\n",
      "iteration 27\n",
      "iteration 28\n",
      "iteration 29\n",
      "iteration 30\n",
      "iteration 31\n",
      "iteration 32\n",
      "iteration 33\n",
      "iteration 34\n",
      "iteration 35\n",
      "iteration 36\n",
      "iteration 37\n",
      "iteration 38\n",
      "iteration 39\n",
      "Document 0 Similarity: 0.4566\n",
      "Document 2 Similarity: 0.4401\n",
      "Document 1 Similarity: 0.4185\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import numpy as np\n",
    "\n",
    "# MeCabの初期化\n",
    "mecab = MeCab.Tagger(\"\")\n",
    "\n",
    "# サンプルの日本語文章\n",
    "documents = [\n",
    "    \"これはテスト文章です。\",\n",
    "    \"もう一つのサンプル文です。\",\n",
    "    \"形態素解析を行い、Doc2Vecモデルを作成します。\"\n",
    "]\n",
    "\n",
    "# 形態素解析を行う関数（特定の品詞を抽出）\n",
    "def tokenize(text):\n",
    "    node = mecab.parseToNode(text)\n",
    "    tokens = []\n",
    "    while node:\n",
    "        feature = node.feature.split(',')\n",
    "        if feature[0] in ['名詞', '動詞', '形容詞']:  # 名詞、動詞、形容詞のみ抽出\n",
    "            tokens.append(node.surface)\n",
    "        node = node.next\n",
    "    return tokens\n",
    "\n",
    "# ドキュメントを形態素解析し、タグ付きドキュメントとして格納\n",
    "tagged_data = []\n",
    "for i, doc in enumerate(documents):\n",
    "    tokens = tokenize(doc)  # ドキュメントをトークン化\n",
    "    tagged_document = TaggedDocument(words=tokens, tags=[str(i)])  # トークンとタグを用いてTaggedDocumentを作成\n",
    "    tagged_data.append(tagged_document)  # tagged_dataリストに追加\n",
    "\n",
    "# Doc2Vecモデルのトレーニング\n",
    "model = Doc2Vec(vector_size=100, alpha=0.025, min_alpha=0.00025, min_count=1, dm=1)\n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "# 40エポックでトレーニング\n",
    "for epoch in range(40):\n",
    "    print(f'iteration {epoch}')\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    model.alpha -= 0.0002\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "# モデルの保存\n",
    "model.save(\"d2v.model\")\n",
    "\n",
    "# モデルのロード\n",
    "model = Doc2Vec.load(\"d2v.model\")\n",
    "\n",
    "# 新しいドキュメントの類似度を計算\n",
    "new_document = \"これは新しい文章です。\"\n",
    "new_vector = model.infer_vector(tokenize(new_document))\n",
    "\n",
    "# 既存のドキュメントと新しいドキュメントの類似度を計算\n",
    "similarities = []\n",
    "for i in range(len(documents)):\n",
    "    doc_vector = model.dv[str(i)]\n",
    "    similarity = np.dot(new_vector, doc_vector) / (np.linalg.norm(new_vector) * np.linalg.norm(doc_vector))\n",
    "    similarities.append((i, similarity))\n",
    "\n",
    "# 類似度の高い順にソート\n",
    "similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 類似度の結果を表示\n",
    "for idx, similarity in similarities:\n",
    "    print(f'Document {idx} Similarity: {similarity:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "モデル作成完了\n"
     ]
    }
   ],
   "source": [
    "# Mecabの初期化\n",
    "mecab = MeCab.Tagger()\n",
    "mecab.parse(\"\")\n",
    "\n",
    "# 学習対象とする青空文庫の作品リスト --- (*1)\n",
    "list = [\n",
    "  {\"auther\":{\n",
    "    \"name\":\"宮澤 賢治\",\n",
    "    \"url\":\"https://www.aozora.gr.jp/cards/000081/files/\"}, \n",
    "   \"books\":[\n",
    "    {\"name\":\"銀河鉄道の夜\",\"zipname\":\"43737_ruby_19028.zip\"},\n",
    "    {\"name\":\"注文の多い料理店\",\"zipname\":\"1927_ruby_17835.zip\"},\n",
    "    {\"name\":\"セロ弾きのゴーシュ\",\"zipname\":\"470_ruby_3987.zip\"},\n",
    "    {\"name\":\"やまなし\",\"zipname\":\"46605_ruby_29758.zip\"},\n",
    "    {\"name\":\"どんぐりと山猫\",\"zipname\":\"43752_ruby_17595.zip\"},\n",
    "  ]},\n",
    "  {\"auther\":{\n",
    "    \"name\":\"芥川 竜之介\",\n",
    "    \"url\":\"https://www.aozora.gr.jp/cards/000879/files/\"}, \n",
    "   \"books\":[\n",
    "    {\"name\":\"羅生門\",\"zipname\":\"127_ruby_150.zip\"},\n",
    "    {\"name\":\"鼻\",\"zipname\":\"42_ruby_154.zip\"},\n",
    "    {\"name\":\"河童\",\"zipname\":\"69_ruby_1321.zip\"},\n",
    "    {\"name\":\"歯車\",\"zipname\":\"42377_ruby_34744.zip\"},\n",
    "    {\"name\":\"老年\",\"zipname\":\"131_ruby_241.zip\"},\n",
    "  ]},\n",
    "  {\"auther\":{\n",
    "    \"name\":\"ポー エドガー・アラン\",\n",
    "    \"url\":\"https://www.aozora.gr.jp/cards/000094/files/\"}, \n",
    "   \"books\":[\n",
    "    {\"name\":\"ウィリアム・ウィルスン\",\"zipname\":\"2523_ruby_19896.zip\"},\n",
    "    {\"name\":\"落穴と振子\",\"zipname\":\"1871_ruby_17551.zip\"},\n",
    "    {\"name\":\"黒猫\",\"zipname\":\"530_ruby_20931.zip\"},\n",
    "    {\"name\":\"群集の人\",\"zipname\":\"56535_ruby_69925.zip\"},\n",
    "    {\"name\":\"沈黙\",\"zipname\":\"56537_ruby_70425.zip\"},\n",
    "  ]},\n",
    "  {\"auther\":{\n",
    "    \"name\":\"紫式部\",\n",
    "    \"url\":\"https://www.aozora.gr.jp/cards/000052/files/\"}, \n",
    "   \"books\":[\n",
    "    {\"name\":\"源氏物語 01 桐壺\",\"zipname\":\"5016_ruby_9746.zip\"},\n",
    "    {\"name\":\"源氏物語 02 帚木\",\"zipname\":\"5017_ruby_9752.zip\"},\n",
    "    {\"name\":\"源氏物語 03 空蝉\",\"zipname\":\"5018_ruby_9754.zip\"},\n",
    "    {\"name\":\"源氏物語 04 夕顔\",\"zipname\":\"5019_ruby_9761.zip\"},\n",
    "    {\"name\":\"源氏物語 05 若紫\",\"zipname\":\"5020_ruby_11253.zip\"},\n",
    "  ]},\n",
    "]\n",
    "\n",
    "# 作品リストを取得してループ処理に渡す --- (*2)\n",
    "def book_list():\n",
    "  for novelist in list:\n",
    "    auther = novelist[\"auther\"]\n",
    "    for book in novelist[\"books\"]:\n",
    "      yield auther, book\n",
    "        \n",
    "# Zipファイルを開き、中の文書を取得する --- (*3)\n",
    "def read_book(auther, book):\n",
    "  zipname = book[\"zipname\"]\n",
    "  # Zipファイルが無ければ取得する\n",
    "  if not os.path.exists(zipname):\n",
    "    req.urlretrieve(auther[\"url\"] + zipname, zipname)\n",
    "  zipname = book[\"zipname\"]\n",
    "  # Zipファイルを開く\n",
    "  with zipfile.ZipFile(zipname,\"r\") as zf:\n",
    "    # Zipファイルに含まれるファイルを開く。\n",
    "    for filename in zf.namelist():\n",
    "      # テキストファイル以外は処理をスキップ\n",
    "      if os.path.splitext(filename)[1] != \".txt\":\n",
    "        continue\n",
    "      with zf.open(filename,\"r\") as f: \n",
    "        # 今回読むファイルはShift-JISなので指定してデコードする\n",
    "        return f.read().decode(\"shift-jis\")\n",
    "\n",
    "# 引数のテキストを分かち書きして配列にする ---(*4)\n",
    "def split_words(text):\n",
    "  node = mecab.parseToNode(text)\n",
    "  wakati_words = []\n",
    "  while node is not None:\n",
    "    hinshi = node.feature.split(\",\")[0]\n",
    "    if  hinshi in [\"名詞\"]:\n",
    "      wakati_words.append(node.surface)\n",
    "    elif hinshi in [\"動詞\", \"形容詞\"]:\n",
    "      wakati_words.append(node.feature.split(\",\")[6])\n",
    "    node = node.next\n",
    "  return wakati_words\n",
    "\n",
    "# 作品リストをDoc2Vecが読めるTaggedDocument形式にし、配列に追加する --- (*5)\n",
    "documents = []\n",
    "# 作品リストをループで回す\n",
    "for auther, book in book_list():\n",
    "  # 作品の文字列を取得\n",
    "  words = read_book(auther, book)\n",
    "  # 作品の文字列を分かち書きに\n",
    "  wakati_words = split_words(words)\n",
    "  # TaggedDocumentの作成　文書=分かち書きにした作品　タグ=作者:作品名\n",
    "  document = TaggedDocument(\n",
    "    wakati_words, [auther[\"name\"] + \":\" + book[\"name\"]])\n",
    "  documents.append(document)\n",
    "    \n",
    "# TaggedDocumentの配列を使ってDoc2Vecの学習モデルを作成 --- (*6)\n",
    "model = Doc2Vec(\n",
    "  documents, dm=0, vector_size=300, window=15, min_count=1)\n",
    "\n",
    "# Doc2Vecの学習モデルを保存\n",
    "model.save('aozora.model')\n",
    "\n",
    "print(\"モデル作成完了\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 「宮沢 賢治:よだかの星」 と似た作品は? ---\n",
      "[('宮澤 賢治:やまなし', 0.906771183013916), ('宮澤 賢治:セロ弾きのゴーシュ', 0.8300795555114746), ('芥川 竜之介:老年', 0.8293884992599487)]\n",
      "\n",
      "--- 「芥川 龍之介:犬と笛」 と似た作品は? ---\n",
      "[('芥川 竜之介:老年', 0.8026412725448608), ('芥川 竜之介:鼻', 0.7319475412368774), ('宮澤 賢治:注文の多い料理店', 0.709476113319397)]\n",
      "\n",
      "--- 「ポー エドガー・アラン:マリー・ロジェエの怪事件」 と似た作品は? ---\n",
      "[('ポー エドガー・アラン:ウィリアム・ウィルスン', 0.879875659942627), ('ポー エドガー・アラン:黒猫', 0.8243330121040344), ('ポー エドガー・アラン:落穴と振子', 0.6893629431724548)]\n",
      "\n",
      "--- 「紫式部:源氏物語 06 末摘花」 と似た作品は? ---\n",
      "[('紫式部:源氏物語 05 若紫', 0.9203005433082581), ('紫式部:源氏物語 03 空蝉', 0.8660159707069397), ('紫式部:源氏物語 02 帚木', 0.8511727452278137)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mecabの初期化\n",
    "mecab = MeCab.Tagger()\n",
    "mecab.parse(\"\")\n",
    "\n",
    "# 保存したDoc2Vec学習モデルを読み込み --- (*7)\n",
    "model = Doc2Vec.load('aozora.model')\n",
    "\n",
    "# 分類用のZipファイルを開き、中の文書を取得する --- (*8)\n",
    "def read_book(url, zipname):\n",
    "  if not os.path.exists(zipname):\n",
    "    req.urlretrieve(url, zipname)\n",
    "\n",
    "  with zipfile.ZipFile(zipname,\"r\") as zf:\n",
    "    for filename in zf.namelist():\n",
    "      with zf.open(filename,\"r\") as f:\n",
    "        return f.read().decode(\"shift-jis\")\n",
    "\n",
    "# 引数のテキストを分かち書きして配列にする\n",
    "def split_words(text):\n",
    "  node = mecab.parseToNode(text)\n",
    "  wakati_words = []\n",
    "  while node is not None:\n",
    "    hinshi = node.feature.split(\",\")[0]\n",
    "    if  hinshi in [\"名詞\"]:\n",
    "      wakati_words.append(node.surface)\n",
    "    elif hinshi in [\"動詞\", \"形容詞\"]:\n",
    "      wakati_words.append(node.feature.split(\",\")[6])\n",
    "    node = node.next\n",
    "  return wakati_words\n",
    "\n",
    "# 引数のタイトル、URLの作品を分類する --- (*9)\n",
    "def similar(title, url):\n",
    "  zipname = url.split(\"/\")[-1]\n",
    "    \n",
    "  words = read_book(url, zipname)\n",
    "  wakati_words = split_words(words)\n",
    "  vector = model.infer_vector(wakati_words)\n",
    "  print(\"--- 「\" + title + '」 と似た作品は? ---')\n",
    "  print(model.dv.most_similar([vector],topn=3))\n",
    "  print(\"\")\n",
    "\n",
    "# 各作家の作品を１つずつ分類 --- (*10)\n",
    "similar(\"宮沢 賢治:よだかの星\",\n",
    "  \"https://www.aozora.gr.jp/cards/000081/files/473_ruby_467.zip\")\n",
    "\n",
    "similar(\"芥川 龍之介:犬と笛\",\n",
    "  \"https://www.aozora.gr.jp/cards/000879/files/56_ruby_845.zip\")\n",
    "\n",
    "similar(\"ポー エドガー・アラン:マリー・ロジェエの怪事件\",\n",
    "  \"https://www.aozora.gr.jp/cards/000094/files/4261_ruby_54182.zip\")\n",
    "\n",
    "similar(\"紫式部:源氏物語 06 末摘花\",\n",
    "  \"https://www.aozora.gr.jp/cards/000052/files/5021_ruby_11106.zip\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
