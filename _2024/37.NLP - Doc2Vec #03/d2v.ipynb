{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import MeCab\n",
    "import zipfile\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['宮沢 賢治', '北原 白秋', '芥川 竜之介', 'カフカ フランツ', 'ポー エドガー・アラン']\n",
      "[['aobikaru_tenkonohateni.txt', 'aoyagikyoyuo_okuru.txt', 'akita_kaido.txt', 'akutaukaberu_asanomizu.txt', 'akegata.txt', 'asanitsuiteno.txt', 'amenimo_makezu.txt', 'arito_kinoko.txt', 'aru_nogakuseino_nisshi.txt', 'igirisu_kaigan.txt'], ['02aino_shishuno_hajimeni.txt', 'asakusa_aika.txt', 'unasaka.txt', 'otsukisama_ikutsu.txt', 'omoide.txt', 'kaihyoto_kumo.txt', 'kage.txt', 'kazami.txt', 'kansono_aki.txt', 'kansono_toki.txt'], ['aidokushono_insho.txt', 'aki.txt', 'akutagawa_ryunosuke_kashu.txt', 'agunino_kami.txt', 'agunino_kami.txt', 'akuma.txt', 'asakusa_koen.txt', 'anikino_yona_kokoromochi.txt', 'anokorono_jibun.txt', 'ababababa.txt'], ['ieno_arujitoshite_kininarukoto.txt', 'kachono_shinpai.txt', 'kafu.txt', 'koteino_shisha.txt', 'saishono_kuno.txt', 'shokeino_hanashi.txt', 'shiro.txt', 'shinpan.txt', 'danjiki_geinin.txt', 'tsumi_kutsu_kibo_oyobi.txt'], ['usher_keno_fukumetsu.txt', 'asshakeno_hokai.txt', 'william_wilson.txt', 'uzushio.txt', 'otoshianato_furiko.txt', 'kuroneko.txt', 'gunshuno_hito.txt', 'koganemushi.txt', 'shimeshi_awase.txt', 'jusanji.txt']]\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 作品の収集\n",
    "# ---------------------------------------------------------\n",
    "def collect_books(author_url, books_count):\n",
    "\n",
    "  author_name = None\n",
    "  book_files = []\n",
    "\n",
    "  ### 作者のページから、作家名を収集 ###\n",
    "  response = requests.get(author_url)\n",
    "  soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "  for tr in soup.find_all('tr'):\n",
    "    header_td = tr.find('td', class_='header')\n",
    "    if header_td and '作家名' in header_td.get_text():\n",
    "      next_td = header_td.find_next_sibling('td')\n",
    "      if next_td:\n",
    "        font_tag = next_td.find('font', size='+2')\n",
    "        if font_tag:\n",
    "          author_name = font_tag.get_text()\n",
    "          break\n",
    "\n",
    "  ### 作者のページから、書籍の URL を収集 ###\n",
    "  book_urls = []\n",
    "  \n",
    "  for ol in soup.find_all('ol'):\n",
    "    for li in soup.find_all('li'):\n",
    "      a_tag = li.find('a', href=True)\n",
    "      if a_tag:\n",
    "        href = a_tag['href']\n",
    "        if (href[0:3] == '../'):\n",
    "          book_urls.append('https://www.aozora.gr.jp/' + href.split('../')[1])\n",
    "\n",
    "  ### 書籍のページから zip ファイルの URL を収集 ###\n",
    "  file_urls = []\n",
    "  file_index = 0\n",
    "\n",
    "  for url in book_urls:\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    for td in soup.find_all('td'):\n",
    "      if ((\"テキストファイル(ルビあり)\" in td.get_text()) or \n",
    "          (\"テキストファイル(ルビなし)\" in td.get_text())):\n",
    "        a_tag = td.find_next('a', href=True)\n",
    "        if a_tag:\n",
    "          href = a_tag['href']\n",
    "          break\n",
    "\n",
    "    url = re.match(r'(https://www\\.aozora\\.gr\\.jp/cards/\\d+/)', url)\n",
    "\n",
    "    if (file_index < books_count):\n",
    "     if (href[0:3] == './'):\n",
    "        file_urls.append(url.group(1) + href.split('./')[1])\n",
    "        file_index = file_index + 1\n",
    "    else:\n",
    "      break\n",
    "\n",
    "  ### zip ファイルをダウンロードして保存 ###\n",
    "  zip_dir = 'zip_' + author_name\n",
    "  os.makedirs(zip_dir, exist_ok=True)\n",
    "\n",
    "  for url in file_urls:\n",
    "    response = requests.get(url)\n",
    "    file_path = os.path.join(zip_dir, url.split('/')[-1])\n",
    "\n",
    "    with open(file_path, 'wb') as file:\n",
    "      file.write(response.content)\n",
    "\n",
    "  ### zip ファイルを解凍してテキストファイルのみを保存 ###\n",
    "  txt_dir = 'txt_' + author_name\n",
    "  os.makedirs(txt_dir, exist_ok=True)\n",
    "\n",
    "  for file in os.listdir(zip_dir):\n",
    "    if file.endswith('.zip'):\n",
    "      file_path = os.path.join(zip_dir, file)\n",
    "\n",
    "      with zipfile.ZipFile(file_path, 'r') as file:\n",
    "        for file_info in file.infolist():\n",
    "          if file_info.filename.endswith('.txt'):\n",
    "            file.extract(file_info, txt_dir)\n",
    "            book_files.append(file_info.filename)\n",
    "\n",
    "  return author_name, book_files\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# メイン（書籍の最初の 10 件をダウンロード）\n",
    "# ---------------------------------------------------------\n",
    "url_list = [\n",
    "  'https://www.aozora.gr.jp/index_pages/person81.html',       # 宮沢 賢治\n",
    "  'https://www.aozora.gr.jp/index_pages/person106.html',      # 北原 白秋\n",
    "  'https://www.aozora.gr.jp/index_pages/person879.html',      # 芥川 竜之介  \n",
    "  'https://www.aozora.gr.jp/index_pages/person1235.html',     # フランツ・カフカ  \n",
    "  'https://www.aozora.gr.jp/index_pages/person94.html'        # エドガー・アラン・ポー  \n",
    "]\n",
    "\n",
    "author_list = []\n",
    "file_list = []\n",
    "\n",
    "for list in url_list:\n",
    "  author, book_files = collect_books(list, 10)                # 書籍を 10 件取集\n",
    "  author_list.append(author)                                  # 作者を保存\n",
    "  file_list.append(book_files)                                # ファイル名を保存\n",
    "\n",
    "print(author_list)\n",
    "print(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# データの作成\n",
    "# ---------------------------------------------------------\n",
    "def create_data(author_name, txt_files):\n",
    "  txt_dir = 'txt_' + author_name\n",
    "  book_list = []\n",
    "  content_list = []\n",
    "\n",
    "  # ファイルの読み込みとコンテンツの保存\n",
    "  for file_name in txt_files:\n",
    "    file_path = os.path.join(txt_dir, file_name)\n",
    "    with open(file_path, 'r', encoding='shift-jis') as file:\n",
    "      book_list.append(file.readline().strip())                   # 書籍名を取得\n",
    "      content_list.append(tokenize(sanitize(file.read())))        # 不要な部分を削除したコンテンツのトークンを取得\n",
    "\n",
    "  return book_list, content_list\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# コンテンツの不要な部分を削除\n",
    "# ---------------------------------------------------------\n",
    "def sanitize(text):\n",
    "  operations = [\n",
    "    lambda text: re.split(r'\\-{5,}', text)[2],\n",
    "    lambda text: re.split(r'底本：', text)[0],\n",
    "    lambda text: re.sub(r'《.+?》', '', text),\n",
    "    lambda text: re.sub(r'［＃.+?］', '', text),\n",
    "    lambda text: text.strip()\n",
    "  ]\n",
    "\n",
    "  for operation in operations:\n",
    "    try:\n",
    "      text = operation(text)\n",
    "    except Exception as e:\n",
    "      pass\n",
    "\n",
    "  return text\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 形態素解析\n",
    "# ---------------------------------------------------------\n",
    "def tokenize(text):\n",
    "  tagger = MeCab.Tagger(\"\")\n",
    "  node = tagger.parseToNode(text)\n",
    "  tokens = []\n",
    "\n",
    "  while node is not None:\n",
    "    part = node.feature.split(\",\")[0]\n",
    "    if part in [\"名詞\", \"動詞\", \"形容詞\"]:\n",
    "      tokens.append(node.surface)\n",
    "    node = node.next\n",
    "\n",
    "  return tokens\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# メイン\n",
    "# ---------------------------------------------------------\n",
    "data = []\n",
    "\n",
    "# データの作成\n",
    "for i in range(len(author_list)):\n",
    "  book_list, content_list = create_data(author_list[i], file_list[i])\n",
    "  for j in range(len(book_list)):\n",
    "    tag = f'{author_list[i]}：{book_list[j]}'\n",
    "    data.append({'tag': tag, 'content': ' '.join(content_list[j])})\n",
    "\n",
    "# TF-IDF モデルの作成\n",
    "corpus = [d['content'] for d in data]\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "宮沢 賢治：〔青びかる天弧のはてに〕：0.8765738157160747\n",
      "宮沢 賢治：青柳教諭を送る：0.05507700755758842\n",
      "北原 白秋：思ひ出：0.028070021138775613\n",
      "芥川 竜之介：芥川龍之介歌集：0.026107850303448032\n",
      "北原 白秋：海豹と雲：0.024870680788239254\n",
      "宮沢 賢治：或る農学生の日誌：0.0235249175486214\n",
      "宮沢 賢治：秋田街道：0.02174688698917026\n",
      "北原 白秋：海阪：0.02088359503897087\n",
      "ポー エドガー・アラン：アッシャー家の崩壊：0.019062835463995256\n",
      "ポー エドガー・アラン：ウィリアム・ウィルスン：0.018969645802563814\n"
     ]
    }
   ],
   "source": [
    "# 新しいテキストのベクトル化と類似度の計算\n",
    "new_text = '青びかる天弧のはてに、きらゝかに町はうかびて、六月のたつきのみちは、いまやはた尽きはてにけり。いさゝかの書籍とセロを、思ふまゝ'\n",
    "new_text_tokens = tokenize(new_text)\n",
    "new_text_vector = vectorizer.transform([' '.join(new_text_tokens)])\n",
    "\n",
    "# コサイン類似度の計算\n",
    "cosine_similarities = cosine_similarity(new_text_vector, tfidf_matrix).flatten()\n",
    "similar_indices = cosine_similarities.argsort()[:-11:-1]\n",
    "\n",
    "# 結果の表示\n",
    "for idx in similar_indices:\n",
    "  print(f\"{data[idx]['tag']}：{cosine_similarities[idx]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
