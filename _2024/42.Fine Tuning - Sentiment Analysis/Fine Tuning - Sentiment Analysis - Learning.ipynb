{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import pickle\n",
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizerFast, BertForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "from datasets import load_dataset\n",
        "\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------\n",
        "# データの読み込み\n",
        "# ---------------------------------------------------------\n",
        "dataset = load_dataset(\"imdb\")\n",
        "train_data = dataset[\"train\"].train_test_split(shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------\n",
        "# トークナイズ\n",
        "# ---------------------------------------------------------\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize(batch):\n",
        "  return tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=512)\n",
        "\n",
        "train_dataset = train_data[\"train\"]\n",
        "eval_dataset = train_data[\"test\"]\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize, batched=True)\n",
        "eval_dataset = eval_dataset.map(tokenize, batched=True)\n",
        "\n",
        "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "eval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------\n",
        "# 評価と学習\n",
        "# ---------------------------------------------------------\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "data_collator = DataCollatorWithPadding(tokenizer)\n",
        "\n",
        "for param in model.bert.encoder.layer[:2].parameters():    # 先頭 2 層を凍結\n",
        "  param.requires_grad = False\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  output_dir = \"./results\",                 # 結果を格納するディレクトリー\n",
        "  logging_dir = \"./logs\",                   # 途中経過のログを格納するディレクトリー\n",
        "  overwrite_output_dir = True,              # ファイルを上書きする\n",
        "  num_train_epochs = 2,                     # エポック数\n",
        "  per_device_train_batch_size = 4,          # 訓練時のバッチサイズ\n",
        "  per_device_eval_batch_size = 4,           # 評価時のバッチサイズ\n",
        "  warmup_steps = 500,                       # 学習系数がこのステップ数で徐々に増加\n",
        "  weight_decay = 0.01,                      # 重みの減衰率\n",
        "  eval_strategy = \"steps\",                  # 訓練中、一定のステップごとに評価\n",
        "  save_safetensors = False                  # safetensors の使用を無効化\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "  model = model,                            # 使用するモデルを指定\n",
        "  args = training_args,                     # TrainingArguments の設定\n",
        "  train_dataset = train_dataset,            # 訓練用のデータ\n",
        "  eval_dataset = eval_dataset,              # 評価用のデータ\n",
        "  data_collator = data_collator             # データコレーターを設定\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------\n",
        "# モデルの保存\n",
        "# ---------------------------------------------------------\n",
        "model_path = \"./model\"\n",
        "\n",
        "if not os.path.exists(model_path):\n",
        "  os.makedirs(model_path)\n",
        "\n",
        "model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
