{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "262f3b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import MeCab\n",
    "import pickle\n",
    "import numpy as np\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c5a8727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# データの作成\n",
    "# ---------------------------------------------------------\n",
    "def create_data(author_name, txt_files):\n",
    "  txt_dir = 'txt_' + author_name\n",
    "  book_list = []\n",
    "  content_list = []\n",
    "\n",
    "  # ファイルの読み込みとコンテンツの保存\n",
    "  for file_name in txt_files:\n",
    "    file_path = os.path.join(txt_dir, file_name)\n",
    "    with open(file_path, 'r', encoding='shift-jis') as file:\n",
    "      book_list.append(file.readline().strip())                   # 書籍名を取得\n",
    "      content_list.append(tokenize(sanitize(file.read())))        # 不要な部分を削除したコンテンツのトークンを取得\n",
    "\n",
    "  return book_list, content_list\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# コンテンツの不要な部分を除外\n",
    "# ---------------------------------------------------------\n",
    "def sanitize(text):\n",
    "  operations = [\n",
    "    lambda text: re.split(r'\\-{5,}', text)[2],\n",
    "    lambda text: re.split(r'底本：', text)[0],\n",
    "    lambda text: re.sub(r'《.+?》', '', text),\n",
    "    lambda text: re.sub(r'［＃.+?］', '', text),\n",
    "    lambda text: text.strip()\n",
    "  ]\n",
    "\n",
    "  for operation in operations:\n",
    "    try:\n",
    "      text = operation(text)\n",
    "    except Exception as e:\n",
    "      pass\n",
    "\n",
    "  return text\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 形態素解析\n",
    "# ---------------------------------------------------------\n",
    "def tokenize(text):\n",
    "  tagger = MeCab.Tagger(\"\")\n",
    "  node = tagger.parseToNode(text)\n",
    "  tokens = []\n",
    "\n",
    "  while node is not None:\n",
    "    part = node.feature.split(\",\")[0]\n",
    "    if part in [\"名詞\", \"動詞\", \"形容詞\"]:\n",
    "      tokens.append(node.surface)\n",
    "    node = node.next\n",
    "\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9861c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# メイン\n",
    "# ---------------------------------------------------------\n",
    "with open(\"book_data.pkl\", \"rb\") as f:          # rb = read-binary\n",
    "    data = pickle.load(f)\n",
    "\n",
    "author_list = data[\"author_list\"]\n",
    "file_list   = data[\"file_list\"]\n",
    "\n",
    "tagged_data = []\n",
    "\n",
    "# データの作成\n",
    "for i in range(len(author_list)):\n",
    "  book_list, content_list = create_data(author_list[i], file_list[i])\n",
    "  for j in range(len(book_list)):\n",
    "    tag = f'{author_list[i]}：{book_list[j]}'\n",
    "    tagged_data.append(TaggedDocument(words=content_list[j], tags=[tag]))\n",
    "\n",
    "# Doc2Vecモデルの作成\n",
    "model = Doc2Vec(vector_size=50, min_count=2, epochs=50)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# モデルの保存\n",
    "model.save(\"d2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "667884a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "宮沢 賢治：〔青びかる天弧のはてに〕：0.8948869705200195\n",
      "宮沢 賢治：青柳教諭を送る：0.8413702249526978\n",
      "北原 白秋：風見：0.8381561040878296\n",
      "宮沢 賢治：〔あくたうかべる朝の水〕：0.8112357258796692\n",
      "宮沢 賢治：〔雨ニモマケズ〕：0.7816504240036011\n",
      "北原 白秋：影：0.7568415999412537\n",
      "芥川 竜之介：芥川龍之介歌集：0.6999874711036682\n",
      "北原 白秋：浅草哀歌：0.6797680258750916\n",
      "芥川 竜之介：愛読書の印象：0.6123363375663757\n",
      "北原 白秋：お月さまいくつ：0.6065266132354736\n"
     ]
    }
   ],
   "source": [
    "# モデルのロード\n",
    "model = Doc2Vec.load(\"d2v.model\")\n",
    "\n",
    "# 新しいテキストのトークン化\n",
    "new_text = '青びかる天弧のはてに、きらゝかに町はうかびて、六月のたつきのみちは、いまやはた尽きはてにけり。いさゝかの書籍とセロを、思ふまゝ'\n",
    "new_text_tokens = tokenize(new_text)\n",
    "new_vector = model.infer_vector(new_text_tokens)\n",
    "\n",
    "# 類似度の計算\n",
    "similar_docs = model.dv.most_similar([new_vector], topn=10)\n",
    "\n",
    "# 結果の表示\n",
    "for label, similarity in similar_docs:\n",
    "  print(f\"{label}：{similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74300156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "宮沢 賢治：〔青びかる天弧のはてに〕：0.8948871493339539\n",
      "宮沢 賢治：青柳教諭を送る：0.8413703441619873\n",
      "北原 白秋：風見：0.8381561040878296\n",
      "宮沢 賢治：〔あくたうかべる朝の水〕：0.811235785484314\n",
      "宮沢 賢治：〔雨ニモマケズ〕：0.7816504240036011\n",
      "北原 白秋：影：0.7568415999412537\n",
      "芥川 竜之介：芥川龍之介歌集：0.6999874711036682\n",
      "北原 白秋：浅草哀歌：0.6797680854797363\n",
      "芥川 竜之介：愛読書の印象：0.6123364567756653\n",
      "北原 白秋：お月さまいくつ：0.6065266728401184\n"
     ]
    }
   ],
   "source": [
    "# モデルのロード\n",
    "model = Doc2Vec.load(\"d2v.model\")\n",
    "\n",
    "# 新しいテキストのトークン化\n",
    "new_text = '青びかる天弧のはてに、きらゝかに町はうかびて、六月のたつきのみちは、いまやはた尽きはてにけり。いさゝかの書籍とセロを、思ふまゝ'\n",
    "new_text_tokens = tokenize(new_text)\n",
    "new_vector = model.infer_vector(new_text_tokens)\n",
    "\n",
    "# すべてのドキュメントベクトルを取得\n",
    "doc_vectors = np.array([model.dv[i] for i in range(len(model.dv))])\n",
    "doc_labels = [model.dv.index_to_key[i] for i in range(len(model.dv))]\n",
    "\n",
    "# コサイン類似度の計算\n",
    "new_vector = new_vector.reshape(1, -1)\n",
    "cosine_similarities = cosine_similarity(new_vector, doc_vectors).flatten()\n",
    "similar_indices = cosine_similarities.argsort()[-10:][::-1]\n",
    "\n",
    "# 結果の表示\n",
    "for idx in similar_indices:\n",
    "  print(f\"{doc_labels[idx]}：{cosine_similarities[idx]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
