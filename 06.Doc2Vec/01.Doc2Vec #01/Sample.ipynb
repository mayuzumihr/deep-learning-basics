{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f95bdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import numpy as np\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94b0b856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 20\n",
      "iteration 21\n",
      "iteration 22\n",
      "iteration 23\n",
      "iteration 24\n",
      "iteration 25\n",
      "iteration 26\n",
      "iteration 27\n",
      "iteration 28\n",
      "iteration 29\n",
      "iteration 30\n",
      "iteration 31\n",
      "iteration 32\n",
      "iteration 33\n",
      "iteration 34\n",
      "iteration 35\n",
      "iteration 36\n",
      "iteration 37\n",
      "iteration 38\n",
      "iteration 39\n",
      "iteration 40\n",
      "iteration 41\n",
      "iteration 42\n",
      "iteration 43\n",
      "iteration 44\n",
      "iteration 45\n",
      "iteration 46\n",
      "iteration 47\n",
      "iteration 48\n",
      "iteration 49\n"
     ]
    }
   ],
   "source": [
    "# MeCabの初期化\n",
    "mecab = MeCab.Tagger(\"\")\n",
    "\n",
    "# サンプルの日本語文章\n",
    "documents = [\n",
    "    \"これはテスト文章です。\",\n",
    "    \"もう一つのサンプル文です。\",\n",
    "    \"形態素解析を行い、Doc2Vecモデルを作成します。\"\n",
    "]\n",
    "\n",
    "# 形態素解析を行う関数（特定の品詞を抽出）\n",
    "def tokenize(text):\n",
    "    node = mecab.parseToNode(text)\n",
    "    tokens = []\n",
    "    while node:\n",
    "        feature = node.feature.split(',')\n",
    "        if feature[0] in ['名詞', '動詞', '形容詞']:  # 名詞、動詞、形容詞のみ抽出\n",
    "            tokens.append(node.surface)\n",
    "        node = node.next\n",
    "    return tokens\n",
    "\n",
    "# ドキュメントを形態素解析し、タグ付きドキュメントとして格納\n",
    "tagged_data = []\n",
    "for i, doc in enumerate(documents):\n",
    "    tokens = tokenize(doc)  # ドキュメントをトークン化\n",
    "    tagged_document = TaggedDocument(words=tokens, tags=[str(i)])  # トークンとタグを用いてTaggedDocumentを作成\n",
    "    tagged_data.append(tagged_document)  # tagged_dataリストに追加\n",
    "\n",
    "# Doc2Vecモデルのトレーニング\n",
    "model = Doc2Vec(vector_size=100, alpha=0.025, min_alpha=0.00025, min_count=1, dm=1)\n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "# 50エポックでトレーニング\n",
    "for epoch in range(50):\n",
    "    print(f'iteration {epoch}')\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    model.alpha -= 0.0002\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "# モデルの保存\n",
    "model.save(\"d2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78ffcc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 2 Similarity: 0.7878\n",
      "Document 1 Similarity: 0.7866\n",
      "Document 0 Similarity: 0.7829\n"
     ]
    }
   ],
   "source": [
    "# モデルのロード\n",
    "model = Doc2Vec.load(\"d2v.model\")\n",
    "\n",
    "# 新しいドキュメントの類似度を計算\n",
    "new_document = \"これは新しい文章です。\"\n",
    "new_vector = model.infer_vector(tokenize(new_document))\n",
    "\n",
    "# 既存のドキュメントと新しいドキュメントの類似度を計算\n",
    "similarities = []\n",
    "for i in range(len(documents)):\n",
    "    doc_vector = model.dv[str(i)]\n",
    "    similarity = np.dot(new_vector, doc_vector) / (np.linalg.norm(new_vector) * np.linalg.norm(doc_vector))\n",
    "    similarities.append((i, similarity))\n",
    "\n",
    "# 類似度の高い順にソート\n",
    "similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 類似度の結果を表示\n",
    "for idx, similarity in similarities:\n",
    "    print(f'Document {idx} Similarity: {similarity:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
